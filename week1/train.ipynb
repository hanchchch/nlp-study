{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1801350/1801350 [00:37<00:00, 47616.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import WikiText103\n",
    "from word2vec import get_word_map\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = WikiText103(\n",
    "    root=\".data\", split=(\"train\", \"valid\", \"test\")\n",
    ")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "word_map = get_word_map(train_dataset, tokenizer, 1_801_350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "projection=tensor([[ 0.6424,  1.8408, -0.2325, -0.3491,  0.3998,  1.1022,  0.1251,  0.0914,\n",
      "         -0.2396, -2.2874,  0.3392, -0.9213,  1.2073,  1.4967,  0.7628, -0.7052,\n",
      "          0.1810, -0.8401, -1.0825, -0.0593,  1.9479,  0.3979,  0.9004, -0.6199,\n",
      "         -0.0453,  0.3946, -1.0494, -0.2865,  0.4672, -0.7395, -0.6748, -1.5855,\n",
      "         -0.9482,  0.8964,  0.8711,  0.1805, -1.1238, -0.8564, -0.3982,  1.8227,\n",
      "          0.6521, -0.8815,  0.1281, -0.4477,  0.0636,  0.3848,  0.4602,  1.0222,\n",
      "          0.7322, -1.8572,  0.6390,  0.9111,  0.7200,  1.5953, -0.6060,  0.3763,\n",
      "          0.2067, -1.5078, -1.5643,  1.5842,  0.0540,  0.9590, -0.5611, -0.3937,\n",
      "          1.2759,  1.9524,  0.1391, -1.3610, -1.3832, -0.3779,  0.5785,  1.6927,\n",
      "         -0.6248, -0.1265,  0.7145,  1.4153,  0.6819,  1.8505, -0.8557, -0.1103,\n",
      "         -0.5612,  0.8984,  0.6653, -1.4993, -0.4308,  1.0676,  0.3320,  0.3359,\n",
      "          0.7244,  0.7599,  0.0370, -1.1108, -1.5609,  0.8165, -0.1038,  2.4269,\n",
      "         -0.7163, -0.3644,  0.9586,  0.2343]], grad_fn=<DivBackward0>)\n",
      "output=tensor([[ 1.6254,  0.5109, -8.0602,  ..., -6.0945, -2.7320,  5.3085]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[5.6944e-20, 1.8683e-20, 3.5404e-24,  ..., 2.5278e-23, 7.2952e-22,\n",
      "         2.2646e-18]], grad_fn=<SoftmaxBackward0>)\n",
      "projection=tensor([[-3.4275e-01, -1.0175e+00, -1.3314e+00,  1.3977e+00,  1.3440e-01,\n",
      "          8.6255e-01, -1.7693e-01,  3.4897e-01, -1.0407e+00, -2.0306e-01,\n",
      "         -4.1961e-01,  1.4251e-01,  5.9425e-01,  1.1366e+00,  3.5729e-01,\n",
      "         -5.4289e-01,  1.0884e+00,  6.8011e-01, -5.7305e-01, -3.0683e-01,\n",
      "          4.2145e-01, -6.9855e-01, -9.0150e-01, -5.3269e-01,  2.6313e-01,\n",
      "          1.0191e+00,  8.1604e-01,  3.6943e-01,  4.3099e-01, -7.8588e-02,\n",
      "          8.6448e-01,  1.1072e+00, -6.0888e-01,  3.1958e-01, -3.0879e-01,\n",
      "         -1.3711e-03, -1.1280e+00, -1.2458e-01,  1.7557e-01, -4.8093e-01,\n",
      "          2.6936e-01, -9.7493e-02,  8.6847e-01, -1.3719e+00,  2.4702e-01,\n",
      "          1.5672e-01, -1.9213e-01,  3.5832e-01,  7.2274e-01,  9.6958e-01,\n",
      "         -1.2006e-01, -6.7562e-01, -4.5641e-01, -8.8459e-01,  3.3493e-01,\n",
      "         -1.0439e+00,  1.0226e-01, -4.9959e-01, -5.1759e-01, -8.9477e-02,\n",
      "          4.5466e-01,  1.0313e+00,  1.4722e+00, -1.2547e+00,  2.2661e-01,\n",
      "          3.1237e-01, -2.8122e-01,  4.5063e-01, -5.3949e-01, -2.1811e+00,\n",
      "         -7.6225e-01,  3.9944e-01,  9.4293e-01, -1.1043e+00, -2.1885e-03,\n",
      "         -2.5760e-01, -4.0214e-01,  4.6547e-01,  9.3339e-05,  3.2739e-01,\n",
      "          1.1291e-01, -1.0600e-01, -2.5938e-01,  3.3457e-01, -4.3597e-01,\n",
      "          8.4539e-02, -9.0730e-01, -3.0105e-01,  4.6711e-01, -2.1495e-01,\n",
      "         -8.4675e-01,  2.8732e-01,  8.2256e-02,  8.0254e-01, -3.6126e-02,\n",
      "         -8.9646e-01,  1.0065e+00, -2.5303e-01, -3.5146e-01,  4.2411e-01]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "output=tensor([[-2.1112,  3.2697,  1.7349,  ..., -0.4339, -7.2209, -8.9676]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[5.3844e-15, 1.1697e-12, 2.5206e-13,  ..., 2.8813e-14, 3.2510e-17,\n",
      "         5.6683e-18]], grad_fn=<SoftmaxBackward0>)\n",
      "projection=tensor([[ 0.0823,  0.5965, -0.4828, -0.0774,  0.5164,  1.5071,  0.4514,  0.3131,\n",
      "         -0.5649, -0.3264, -0.3879, -0.1261,  0.5224,  0.8080,  0.3037, -0.8372,\n",
      "          0.4048,  0.4684, -0.5626,  0.0041,  0.0059,  0.5052,  0.2762, -0.7040,\n",
      "         -0.6583,  0.7434,  0.0364, -0.1468,  1.1923, -0.2094,  0.3578, -0.1132,\n",
      "         -0.5202,  0.0505,  0.1572,  0.4843, -0.3992, -0.9727, -0.2917,  0.5138,\n",
      "          0.7749, -0.2172,  1.1445, -0.1251, -0.2780,  0.0773,  0.3459,  1.2962,\n",
      "          1.0131, -0.1063,  0.3185, -0.4910, -0.0936,  0.4363, -0.4326,  0.4560,\n",
      "         -0.2235, -0.5223, -0.4281,  0.8629,  0.1567,  0.9137,  0.5597, -1.0156,\n",
      "         -0.1025,  0.4284,  0.0912,  0.1316, -0.6216, -0.7116, -0.3848, -0.3642,\n",
      "          0.0617, -0.2562, -0.2949,  0.4660, -0.1825,  0.6164, -0.3056,  0.6982,\n",
      "         -0.9114,  0.8892, -0.6122, -0.5247, -0.7964,  0.7677, -0.6261, -0.6138,\n",
      "         -0.1404,  0.2541, -0.0077, -0.3614, -0.6240,  0.2585, -0.6490,  0.6982,\n",
      "         -0.5003, -0.0172,  0.1548, -0.0474]], grad_fn=<DivBackward0>)\n",
      "output=tensor([[-0.1728,  0.0565, -4.3828,  ...,  0.4216, -3.4474,  3.3443]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[1.6581e-12, 2.0855e-12, 2.4617e-14,  ..., 3.0045e-12, 6.2731e-14,\n",
      "         5.5857e-11]], grad_fn=<SoftmaxBackward0>)\n",
      "projection=tensor([[-0.0144, -0.0647, -0.9651,  0.8154,  0.2229,  0.9424, -0.0763,  0.2631,\n",
      "         -0.7737, -0.8978, -0.1667, -0.2121,  0.7986,  1.2566,  0.4924, -0.5970,\n",
      "          0.7860,  0.1734, -0.7429, -0.2243,  0.9303, -0.3331, -0.3009, -0.5618,\n",
      "          0.1603,  0.8110,  0.1942,  0.1508,  0.4430, -0.2989,  0.3514,  0.2096,\n",
      "         -0.7220,  0.5119,  0.0845,  0.0593, -1.1266, -0.3685, -0.0157,  0.2869,\n",
      "          0.3969, -0.3588,  0.6217, -1.0638,  0.1859,  0.2327,  0.0253,  0.5796,\n",
      "          0.7259,  0.0273,  0.1330, -0.1467, -0.0643, -0.0579,  0.0213, -0.5705,\n",
      "          0.1371, -0.8357, -0.8665,  0.4684,  0.3211,  1.0072,  0.7944, -0.9677,\n",
      "          0.5764,  0.8590, -0.1411, -0.1533, -0.8207, -1.5801, -0.3153,  0.8305,\n",
      "          0.4203, -0.7784,  0.2367,  0.3000, -0.0408,  0.9271, -0.2852,  0.1815,\n",
      "         -0.1118,  0.2288,  0.0488, -0.2767, -0.4342,  0.4122, -0.4942, -0.0887,\n",
      "          0.5529,  0.1100, -0.5522, -0.1787, -0.4654,  0.8072, -0.0587,  0.2113,\n",
      "          0.4322, -0.2902,  0.0852,  0.3608]], grad_fn=<DivBackward0>)\n",
      "output=tensor([[-0.8657,  2.3501, -1.5301,  ..., -2.3208, -5.7246, -4.2089]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[7.0548e-13, 1.7583e-11, 3.6301e-13,  ..., 1.6465e-13, 5.4735e-15,\n",
      "         2.4919e-14]], grad_fn=<SoftmaxBackward0>)\n",
      "projection=tensor([[-0.0021, -0.7965,  0.2911,  0.7842, -0.1643,  0.6791,  0.0189,  0.6290,\n",
      "          0.0433, -1.4789, -0.0103, -0.0815,  0.1709,  0.6675, -0.1816,  0.0095,\n",
      "          0.6374,  0.6767,  0.2948, -0.4644,  0.4407, -0.9223, -1.0219,  0.4320,\n",
      "         -0.0698, -0.0980,  0.3786,  0.4925,  0.5985,  1.4086, -0.0050,  1.0340,\n",
      "         -0.2553,  0.1875,  0.3156,  1.1405, -0.8534,  0.0463,  0.1457,  0.3403,\n",
      "          1.1040, -0.5591, -0.1573, -1.0634,  0.3838, -0.3621, -0.0457,  0.8505,\n",
      "          0.1336,  0.4318,  0.7096, -0.3824, -1.1243, -0.4249,  1.1974, -0.1171,\n",
      "         -0.0180, -1.0505, -0.0561,  0.1272,  0.7632,  0.6226,  1.6027, -0.9008,\n",
      "          0.4042,  0.2226, -0.7852,  0.5610,  0.0268, -0.6063,  0.3370, -0.4504,\n",
      "          0.5175, -0.4652,  0.0862,  0.4586,  0.1523, -1.0420, -0.0213,  0.4570,\n",
      "         -0.5747,  0.9069, -1.0162,  1.0792, -0.2011,  0.8029, -1.6200, -1.7439,\n",
      "         -0.1337, -0.5469, -0.4852, -0.2576,  0.0782, -0.2286, -0.0822, -0.6997,\n",
      "          0.5860, -0.3043,  0.0312,  0.1452]], grad_fn=<DivBackward0>)\n",
      "output=tensor([[ -3.1695,   9.7648,   3.5016,  ...,   3.3907, -11.5934,  -0.2474]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[1.0981e-15, 4.5488e-10, 8.6663e-13,  ..., 7.7570e-13, 2.4108e-19,\n",
      "         2.0401e-14]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(word_map)\n",
    "\n",
    "for i, sentence in enumerate(train_dataset):\n",
    "    words = tokenizer(sentence)\n",
    "\n",
    "    for word_index, _ in enumerate(words):\n",
    "        print(model(words, word_index))\n",
    "\n",
    "    if i == 2:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
